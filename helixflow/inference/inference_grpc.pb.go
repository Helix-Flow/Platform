// Code generated by protoc-gen-go-grpc. DO NOT EDIT.
// versions:
// - protoc-gen-go-grpc v1.6.0
// - protoc             v3.21.12
// source: proto/inference.proto

package inference

import (
	context "context"
	grpc "google.golang.org/grpc"
	codes "google.golang.org/grpc/codes"
	status "google.golang.org/grpc/status"
)

// This is a compile-time assertion to ensure that this generated file
// is compatible with the grpc package it is being compiled against.
// Requires gRPC-Go v1.64.0 or later.
const _ = grpc.SupportPackageIsVersion9

const (
	InferenceService_Inference_FullMethodName       = "/helixflow.inference.InferenceService/Inference"
	InferenceService_StreamInference_FullMethodName = "/helixflow.inference.InferenceService/StreamInference"
	InferenceService_GetModelStatus_FullMethodName  = "/helixflow.inference.InferenceService/GetModelStatus"
	InferenceService_ListModels_FullMethodName      = "/helixflow.inference.InferenceService/ListModels"
	InferenceService_LoadModel_FullMethodName       = "/helixflow.inference.InferenceService/LoadModel"
	InferenceService_UnloadModel_FullMethodName     = "/helixflow.inference.InferenceService/UnloadModel"
	InferenceService_GetSystemStatus_FullMethodName = "/helixflow.inference.InferenceService/GetSystemStatus"
)

// InferenceServiceClient is the client API for InferenceService service.
//
// For semantics around ctx use and closing/ending streaming RPCs, please refer to https://pkg.go.dev/google.golang.org/grpc/?tab=doc#ClientConn.NewStream.
//
// Inference Service for AI model operations
type InferenceServiceClient interface {
	// Standard inference request
	Inference(ctx context.Context, in *InferenceRequest, opts ...grpc.CallOption) (*InferenceResponse, error)
	// Streaming inference for real-time responses
	StreamInference(ctx context.Context, in *InferenceRequest, opts ...grpc.CallOption) (grpc.ServerStreamingClient[InferenceChunk], error)
	// Get model status and information
	GetModelStatus(ctx context.Context, in *ModelStatusRequest, opts ...grpc.CallOption) (*ModelStatusResponse, error)
	// List available models
	ListModels(ctx context.Context, in *ListModelsRequest, opts ...grpc.CallOption) (*ListModelsResponse, error)
	// Load a model into memory
	LoadModel(ctx context.Context, in *LoadModelRequest, opts ...grpc.CallOption) (*LoadModelResponse, error)
	// Unload a model from memory
	UnloadModel(ctx context.Context, in *UnloadModelRequest, opts ...grpc.CallOption) (*UnloadModelResponse, error)
	// Get system status
	GetSystemStatus(ctx context.Context, in *SystemStatusRequest, opts ...grpc.CallOption) (*SystemStatusResponse, error)
}

type inferenceServiceClient struct {
	cc grpc.ClientConnInterface
}

func NewInferenceServiceClient(cc grpc.ClientConnInterface) InferenceServiceClient {
	return &inferenceServiceClient{cc}
}

func (c *inferenceServiceClient) Inference(ctx context.Context, in *InferenceRequest, opts ...grpc.CallOption) (*InferenceResponse, error) {
	cOpts := append([]grpc.CallOption{grpc.StaticMethod()}, opts...)
	out := new(InferenceResponse)
	err := c.cc.Invoke(ctx, InferenceService_Inference_FullMethodName, in, out, cOpts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *inferenceServiceClient) StreamInference(ctx context.Context, in *InferenceRequest, opts ...grpc.CallOption) (grpc.ServerStreamingClient[InferenceChunk], error) {
	cOpts := append([]grpc.CallOption{grpc.StaticMethod()}, opts...)
	stream, err := c.cc.NewStream(ctx, &InferenceService_ServiceDesc.Streams[0], InferenceService_StreamInference_FullMethodName, cOpts...)
	if err != nil {
		return nil, err
	}
	x := &grpc.GenericClientStream[InferenceRequest, InferenceChunk]{ClientStream: stream}
	if err := x.ClientStream.SendMsg(in); err != nil {
		return nil, err
	}
	if err := x.ClientStream.CloseSend(); err != nil {
		return nil, err
	}
	return x, nil
}

// This type alias is provided for backwards compatibility with existing code that references the prior non-generic stream type by name.
type InferenceService_StreamInferenceClient = grpc.ServerStreamingClient[InferenceChunk]

func (c *inferenceServiceClient) GetModelStatus(ctx context.Context, in *ModelStatusRequest, opts ...grpc.CallOption) (*ModelStatusResponse, error) {
	cOpts := append([]grpc.CallOption{grpc.StaticMethod()}, opts...)
	out := new(ModelStatusResponse)
	err := c.cc.Invoke(ctx, InferenceService_GetModelStatus_FullMethodName, in, out, cOpts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *inferenceServiceClient) ListModels(ctx context.Context, in *ListModelsRequest, opts ...grpc.CallOption) (*ListModelsResponse, error) {
	cOpts := append([]grpc.CallOption{grpc.StaticMethod()}, opts...)
	out := new(ListModelsResponse)
	err := c.cc.Invoke(ctx, InferenceService_ListModels_FullMethodName, in, out, cOpts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *inferenceServiceClient) LoadModel(ctx context.Context, in *LoadModelRequest, opts ...grpc.CallOption) (*LoadModelResponse, error) {
	cOpts := append([]grpc.CallOption{grpc.StaticMethod()}, opts...)
	out := new(LoadModelResponse)
	err := c.cc.Invoke(ctx, InferenceService_LoadModel_FullMethodName, in, out, cOpts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *inferenceServiceClient) UnloadModel(ctx context.Context, in *UnloadModelRequest, opts ...grpc.CallOption) (*UnloadModelResponse, error) {
	cOpts := append([]grpc.CallOption{grpc.StaticMethod()}, opts...)
	out := new(UnloadModelResponse)
	err := c.cc.Invoke(ctx, InferenceService_UnloadModel_FullMethodName, in, out, cOpts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *inferenceServiceClient) GetSystemStatus(ctx context.Context, in *SystemStatusRequest, opts ...grpc.CallOption) (*SystemStatusResponse, error) {
	cOpts := append([]grpc.CallOption{grpc.StaticMethod()}, opts...)
	out := new(SystemStatusResponse)
	err := c.cc.Invoke(ctx, InferenceService_GetSystemStatus_FullMethodName, in, out, cOpts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

// InferenceServiceServer is the server API for InferenceService service.
// All implementations must embed UnimplementedInferenceServiceServer
// for forward compatibility.
//
// Inference Service for AI model operations
type InferenceServiceServer interface {
	// Standard inference request
	Inference(context.Context, *InferenceRequest) (*InferenceResponse, error)
	// Streaming inference for real-time responses
	StreamInference(*InferenceRequest, grpc.ServerStreamingServer[InferenceChunk]) error
	// Get model status and information
	GetModelStatus(context.Context, *ModelStatusRequest) (*ModelStatusResponse, error)
	// List available models
	ListModels(context.Context, *ListModelsRequest) (*ListModelsResponse, error)
	// Load a model into memory
	LoadModel(context.Context, *LoadModelRequest) (*LoadModelResponse, error)
	// Unload a model from memory
	UnloadModel(context.Context, *UnloadModelRequest) (*UnloadModelResponse, error)
	// Get system status
	GetSystemStatus(context.Context, *SystemStatusRequest) (*SystemStatusResponse, error)
	mustEmbedUnimplementedInferenceServiceServer()
}

// UnimplementedInferenceServiceServer must be embedded to have
// forward compatible implementations.
//
// NOTE: this should be embedded by value instead of pointer to avoid a nil
// pointer dereference when methods are called.
type UnimplementedInferenceServiceServer struct{}

func (UnimplementedInferenceServiceServer) Inference(context.Context, *InferenceRequest) (*InferenceResponse, error) {
	return nil, status.Error(codes.Unimplemented, "method Inference not implemented")
}
func (UnimplementedInferenceServiceServer) StreamInference(*InferenceRequest, grpc.ServerStreamingServer[InferenceChunk]) error {
	return status.Error(codes.Unimplemented, "method StreamInference not implemented")
}
func (UnimplementedInferenceServiceServer) GetModelStatus(context.Context, *ModelStatusRequest) (*ModelStatusResponse, error) {
	return nil, status.Error(codes.Unimplemented, "method GetModelStatus not implemented")
}
func (UnimplementedInferenceServiceServer) ListModels(context.Context, *ListModelsRequest) (*ListModelsResponse, error) {
	return nil, status.Error(codes.Unimplemented, "method ListModels not implemented")
}
func (UnimplementedInferenceServiceServer) LoadModel(context.Context, *LoadModelRequest) (*LoadModelResponse, error) {
	return nil, status.Error(codes.Unimplemented, "method LoadModel not implemented")
}
func (UnimplementedInferenceServiceServer) UnloadModel(context.Context, *UnloadModelRequest) (*UnloadModelResponse, error) {
	return nil, status.Error(codes.Unimplemented, "method UnloadModel not implemented")
}
func (UnimplementedInferenceServiceServer) GetSystemStatus(context.Context, *SystemStatusRequest) (*SystemStatusResponse, error) {
	return nil, status.Error(codes.Unimplemented, "method GetSystemStatus not implemented")
}
func (UnimplementedInferenceServiceServer) mustEmbedUnimplementedInferenceServiceServer() {}
func (UnimplementedInferenceServiceServer) testEmbeddedByValue()                          {}

// UnsafeInferenceServiceServer may be embedded to opt out of forward compatibility for this service.
// Use of this interface is not recommended, as added methods to InferenceServiceServer will
// result in compilation errors.
type UnsafeInferenceServiceServer interface {
	mustEmbedUnimplementedInferenceServiceServer()
}

func RegisterInferenceServiceServer(s grpc.ServiceRegistrar, srv InferenceServiceServer) {
	// If the following call panics, it indicates UnimplementedInferenceServiceServer was
	// embedded by pointer and is nil.  This will cause panics if an
	// unimplemented method is ever invoked, so we test this at initialization
	// time to prevent it from happening at runtime later due to I/O.
	if t, ok := srv.(interface{ testEmbeddedByValue() }); ok {
		t.testEmbeddedByValue()
	}
	s.RegisterService(&InferenceService_ServiceDesc, srv)
}

func _InferenceService_Inference_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(InferenceRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(InferenceServiceServer).Inference(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: InferenceService_Inference_FullMethodName,
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(InferenceServiceServer).Inference(ctx, req.(*InferenceRequest))
	}
	return interceptor(ctx, in, info, handler)
}

func _InferenceService_StreamInference_Handler(srv interface{}, stream grpc.ServerStream) error {
	m := new(InferenceRequest)
	if err := stream.RecvMsg(m); err != nil {
		return err
	}
	return srv.(InferenceServiceServer).StreamInference(m, &grpc.GenericServerStream[InferenceRequest, InferenceChunk]{ServerStream: stream})
}

// This type alias is provided for backwards compatibility with existing code that references the prior non-generic stream type by name.
type InferenceService_StreamInferenceServer = grpc.ServerStreamingServer[InferenceChunk]

func _InferenceService_GetModelStatus_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(ModelStatusRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(InferenceServiceServer).GetModelStatus(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: InferenceService_GetModelStatus_FullMethodName,
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(InferenceServiceServer).GetModelStatus(ctx, req.(*ModelStatusRequest))
	}
	return interceptor(ctx, in, info, handler)
}

func _InferenceService_ListModels_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(ListModelsRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(InferenceServiceServer).ListModels(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: InferenceService_ListModels_FullMethodName,
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(InferenceServiceServer).ListModels(ctx, req.(*ListModelsRequest))
	}
	return interceptor(ctx, in, info, handler)
}

func _InferenceService_LoadModel_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(LoadModelRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(InferenceServiceServer).LoadModel(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: InferenceService_LoadModel_FullMethodName,
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(InferenceServiceServer).LoadModel(ctx, req.(*LoadModelRequest))
	}
	return interceptor(ctx, in, info, handler)
}

func _InferenceService_UnloadModel_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(UnloadModelRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(InferenceServiceServer).UnloadModel(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: InferenceService_UnloadModel_FullMethodName,
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(InferenceServiceServer).UnloadModel(ctx, req.(*UnloadModelRequest))
	}
	return interceptor(ctx, in, info, handler)
}

func _InferenceService_GetSystemStatus_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(SystemStatusRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(InferenceServiceServer).GetSystemStatus(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: InferenceService_GetSystemStatus_FullMethodName,
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(InferenceServiceServer).GetSystemStatus(ctx, req.(*SystemStatusRequest))
	}
	return interceptor(ctx, in, info, handler)
}

// InferenceService_ServiceDesc is the grpc.ServiceDesc for InferenceService service.
// It's only intended for direct use with grpc.RegisterService,
// and not to be introspected or modified (even as a copy)
var InferenceService_ServiceDesc = grpc.ServiceDesc{
	ServiceName: "helixflow.inference.InferenceService",
	HandlerType: (*InferenceServiceServer)(nil),
	Methods: []grpc.MethodDesc{
		{
			MethodName: "Inference",
			Handler:    _InferenceService_Inference_Handler,
		},
		{
			MethodName: "GetModelStatus",
			Handler:    _InferenceService_GetModelStatus_Handler,
		},
		{
			MethodName: "ListModels",
			Handler:    _InferenceService_ListModels_Handler,
		},
		{
			MethodName: "LoadModel",
			Handler:    _InferenceService_LoadModel_Handler,
		},
		{
			MethodName: "UnloadModel",
			Handler:    _InferenceService_UnloadModel_Handler,
		},
		{
			MethodName: "GetSystemStatus",
			Handler:    _InferenceService_GetSystemStatus_Handler,
		},
	},
	Streams: []grpc.StreamDesc{
		{
			StreamName:    "StreamInference",
			Handler:       _InferenceService_StreamInference_Handler,
			ServerStreams: true,
		},
	},
	Metadata: "proto/inference.proto",
}
