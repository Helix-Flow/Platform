syntax = "proto3";

package helixflow.inference;

service InferenceService {
  rpc SubmitInference (InferenceRequest) returns (InferenceResponse);
  rpc GetInferenceStatus (StatusRequest) returns (StatusResponse);
  rpc StreamInference (InferenceRequest) returns (stream InferenceChunk);
}

message InferenceRequest {
  string model = 1;
  repeated ChatMessage messages = 2;
  int32 max_tokens = 3;
  bool stream = 4;
  string user_id = 5;
}

message ChatMessage {
  string role = 1;
  string content = 2;
}

message InferenceResponse {
  string job_id = 1;
  string status = 2;
}

message StatusRequest {
  string job_id = 1;
}

message StatusResponse {
  string job_id = 1;
  string status = 2;
  string result = 3;
}

message InferenceChunk {
  string content = 1;
  bool finished = 2;
}