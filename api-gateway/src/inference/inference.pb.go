// Code generated by protoc-gen-go. DO NOT EDIT.
// versions:
// 	protoc-gen-go v1.36.11
// 	protoc        v3.21.12
// source: proto/inference.proto

package inference

import (
	protoreflect "google.golang.org/protobuf/reflect/protoreflect"
	protoimpl "google.golang.org/protobuf/runtime/protoimpl"
	reflect "reflect"
	sync "sync"
	unsafe "unsafe"
)

const (
	// Verify that this generated code is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(20 - protoimpl.MinVersion)
	// Verify that runtime/protoimpl is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(protoimpl.MaxVersion - 20)
)

// Model status enum
type ModelStatus int32

const (
	ModelStatus_MODEL_STATUS_UNKNOWN  ModelStatus = 0
	ModelStatus_MODEL_STATUS_LOADING  ModelStatus = 1
	ModelStatus_MODEL_STATUS_READY    ModelStatus = 2
	ModelStatus_MODEL_STATUS_ERROR    ModelStatus = 3
	ModelStatus_MODEL_STATUS_UNLOADED ModelStatus = 4
)

// Enum value maps for ModelStatus.
var (
	ModelStatus_name = map[int32]string{
		0: "MODEL_STATUS_UNKNOWN",
		1: "MODEL_STATUS_LOADING",
		2: "MODEL_STATUS_READY",
		3: "MODEL_STATUS_ERROR",
		4: "MODEL_STATUS_UNLOADED",
	}
	ModelStatus_value = map[string]int32{
		"MODEL_STATUS_UNKNOWN":  0,
		"MODEL_STATUS_LOADING":  1,
		"MODEL_STATUS_READY":    2,
		"MODEL_STATUS_ERROR":    3,
		"MODEL_STATUS_UNLOADED": 4,
	}
)

func (x ModelStatus) Enum() *ModelStatus {
	p := new(ModelStatus)
	*p = x
	return p
}

func (x ModelStatus) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (ModelStatus) Descriptor() protoreflect.EnumDescriptor {
	return file_proto_inference_proto_enumTypes[0].Descriptor()
}

func (ModelStatus) Type() protoreflect.EnumType {
	return &file_proto_inference_proto_enumTypes[0]
}

func (x ModelStatus) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use ModelStatus.Descriptor instead.
func (ModelStatus) EnumDescriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{0}
}

// Request message for inference
type InferenceRequest struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	ModelId       string                 `protobuf:"bytes,1,opt,name=model_id,json=modelId,proto3" json:"model_id,omitempty"`
	UserId        string                 `protobuf:"bytes,2,opt,name=user_id,json=userId,proto3" json:"user_id,omitempty"`
	Messages      []*ChatMessage         `protobuf:"bytes,3,rep,name=messages,proto3" json:"messages,omitempty"`
	MaxTokens     int32                  `protobuf:"varint,4,opt,name=max_tokens,json=maxTokens,proto3" json:"max_tokens,omitempty"`
	Temperature   float64                `protobuf:"fixed64,5,opt,name=temperature,proto3" json:"temperature,omitempty"`
	TopP          float64                `protobuf:"fixed64,6,opt,name=top_p,json=topP,proto3" json:"top_p,omitempty"`
	Stop          []string               `protobuf:"bytes,7,rep,name=stop,proto3" json:"stop,omitempty"`
	Stream        bool                   `protobuf:"varint,8,opt,name=stream,proto3" json:"stream,omitempty"`
	Metadata      map[string]string      `protobuf:"bytes,9,rep,name=metadata,proto3" json:"metadata,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *InferenceRequest) Reset() {
	*x = InferenceRequest{}
	mi := &file_proto_inference_proto_msgTypes[0]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *InferenceRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*InferenceRequest) ProtoMessage() {}

func (x *InferenceRequest) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[0]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use InferenceRequest.ProtoReflect.Descriptor instead.
func (*InferenceRequest) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{0}
}

func (x *InferenceRequest) GetModelId() string {
	if x != nil {
		return x.ModelId
	}
	return ""
}

func (x *InferenceRequest) GetUserId() string {
	if x != nil {
		return x.UserId
	}
	return ""
}

func (x *InferenceRequest) GetMessages() []*ChatMessage {
	if x != nil {
		return x.Messages
	}
	return nil
}

func (x *InferenceRequest) GetMaxTokens() int32 {
	if x != nil {
		return x.MaxTokens
	}
	return 0
}

func (x *InferenceRequest) GetTemperature() float64 {
	if x != nil {
		return x.Temperature
	}
	return 0
}

func (x *InferenceRequest) GetTopP() float64 {
	if x != nil {
		return x.TopP
	}
	return 0
}

func (x *InferenceRequest) GetStop() []string {
	if x != nil {
		return x.Stop
	}
	return nil
}

func (x *InferenceRequest) GetStream() bool {
	if x != nil {
		return x.Stream
	}
	return false
}

func (x *InferenceRequest) GetMetadata() map[string]string {
	if x != nil {
		return x.Metadata
	}
	return nil
}

// Chat message structure
type ChatMessage struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	Role          string                 `protobuf:"bytes,1,opt,name=role,proto3" json:"role,omitempty"` // "user", "assistant", "system"
	Content       string                 `protobuf:"bytes,2,opt,name=content,proto3" json:"content,omitempty"`
	Name          string                 `protobuf:"bytes,3,opt,name=name,proto3" json:"name,omitempty"` // Optional name for the message
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ChatMessage) Reset() {
	*x = ChatMessage{}
	mi := &file_proto_inference_proto_msgTypes[1]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ChatMessage) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ChatMessage) ProtoMessage() {}

func (x *ChatMessage) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[1]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ChatMessage.ProtoReflect.Descriptor instead.
func (*ChatMessage) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{1}
}

func (x *ChatMessage) GetRole() string {
	if x != nil {
		return x.Role
	}
	return ""
}

func (x *ChatMessage) GetContent() string {
	if x != nil {
		return x.Content
	}
	return ""
}

func (x *ChatMessage) GetName() string {
	if x != nil {
		return x.Name
	}
	return ""
}

// Response message for inference
type InferenceResponse struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	Id            string                 `protobuf:"bytes,1,opt,name=id,proto3" json:"id,omitempty"`
	Object        string                 `protobuf:"bytes,2,opt,name=object,proto3" json:"object,omitempty"`
	Created       int64                  `protobuf:"varint,3,opt,name=created,proto3" json:"created,omitempty"`
	Model         string                 `protobuf:"bytes,4,opt,name=model,proto3" json:"model,omitempty"`
	Choices       []*Choice              `protobuf:"bytes,5,rep,name=choices,proto3" json:"choices,omitempty"`
	Usage         *Usage                 `protobuf:"bytes,6,opt,name=usage,proto3" json:"usage,omitempty"`
	FinishReason  string                 `protobuf:"bytes,7,opt,name=finish_reason,json=finishReason,proto3" json:"finish_reason,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *InferenceResponse) Reset() {
	*x = InferenceResponse{}
	mi := &file_proto_inference_proto_msgTypes[2]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *InferenceResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*InferenceResponse) ProtoMessage() {}

func (x *InferenceResponse) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[2]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use InferenceResponse.ProtoReflect.Descriptor instead.
func (*InferenceResponse) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{2}
}

func (x *InferenceResponse) GetId() string {
	if x != nil {
		return x.Id
	}
	return ""
}

func (x *InferenceResponse) GetObject() string {
	if x != nil {
		return x.Object
	}
	return ""
}

func (x *InferenceResponse) GetCreated() int64 {
	if x != nil {
		return x.Created
	}
	return 0
}

func (x *InferenceResponse) GetModel() string {
	if x != nil {
		return x.Model
	}
	return ""
}

func (x *InferenceResponse) GetChoices() []*Choice {
	if x != nil {
		return x.Choices
	}
	return nil
}

func (x *InferenceResponse) GetUsage() *Usage {
	if x != nil {
		return x.Usage
	}
	return nil
}

func (x *InferenceResponse) GetFinishReason() string {
	if x != nil {
		return x.FinishReason
	}
	return ""
}

// Streaming response chunk
type InferenceChunk struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	Id            string                 `protobuf:"bytes,1,opt,name=id,proto3" json:"id,omitempty"`
	Object        string                 `protobuf:"bytes,2,opt,name=object,proto3" json:"object,omitempty"`
	Created       int64                  `protobuf:"varint,3,opt,name=created,proto3" json:"created,omitempty"`
	Model         string                 `protobuf:"bytes,4,opt,name=model,proto3" json:"model,omitempty"`
	Choices       []*Choice              `protobuf:"bytes,5,rep,name=choices,proto3" json:"choices,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *InferenceChunk) Reset() {
	*x = InferenceChunk{}
	mi := &file_proto_inference_proto_msgTypes[3]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *InferenceChunk) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*InferenceChunk) ProtoMessage() {}

func (x *InferenceChunk) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[3]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use InferenceChunk.ProtoReflect.Descriptor instead.
func (*InferenceChunk) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{3}
}

func (x *InferenceChunk) GetId() string {
	if x != nil {
		return x.Id
	}
	return ""
}

func (x *InferenceChunk) GetObject() string {
	if x != nil {
		return x.Object
	}
	return ""
}

func (x *InferenceChunk) GetCreated() int64 {
	if x != nil {
		return x.Created
	}
	return 0
}

func (x *InferenceChunk) GetModel() string {
	if x != nil {
		return x.Model
	}
	return ""
}

func (x *InferenceChunk) GetChoices() []*Choice {
	if x != nil {
		return x.Choices
	}
	return nil
}

// Choice in response
type Choice struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	Index         int32                  `protobuf:"varint,1,opt,name=index,proto3" json:"index,omitempty"`
	Message       *ChatMessage           `protobuf:"bytes,2,opt,name=message,proto3" json:"message,omitempty"`
	FinishReason  string                 `protobuf:"bytes,3,opt,name=finish_reason,json=finishReason,proto3" json:"finish_reason,omitempty"`
	Delta         *Delta                 `protobuf:"bytes,4,opt,name=delta,proto3" json:"delta,omitempty"` // For streaming responses
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *Choice) Reset() {
	*x = Choice{}
	mi := &file_proto_inference_proto_msgTypes[4]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *Choice) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*Choice) ProtoMessage() {}

func (x *Choice) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[4]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use Choice.ProtoReflect.Descriptor instead.
func (*Choice) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{4}
}

func (x *Choice) GetIndex() int32 {
	if x != nil {
		return x.Index
	}
	return 0
}

func (x *Choice) GetMessage() *ChatMessage {
	if x != nil {
		return x.Message
	}
	return nil
}

func (x *Choice) GetFinishReason() string {
	if x != nil {
		return x.FinishReason
	}
	return ""
}

func (x *Choice) GetDelta() *Delta {
	if x != nil {
		return x.Delta
	}
	return nil
}

// Delta for streaming responses
type Delta struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	Role          string                 `protobuf:"bytes,1,opt,name=role,proto3" json:"role,omitempty"`
	Content       string                 `protobuf:"bytes,2,opt,name=content,proto3" json:"content,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *Delta) Reset() {
	*x = Delta{}
	mi := &file_proto_inference_proto_msgTypes[5]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *Delta) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*Delta) ProtoMessage() {}

func (x *Delta) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[5]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use Delta.ProtoReflect.Descriptor instead.
func (*Delta) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{5}
}

func (x *Delta) GetRole() string {
	if x != nil {
		return x.Role
	}
	return ""
}

func (x *Delta) GetContent() string {
	if x != nil {
		return x.Content
	}
	return ""
}

// Token usage information
type Usage struct {
	state            protoimpl.MessageState `protogen:"open.v1"`
	PromptTokens     int32                  `protobuf:"varint,1,opt,name=prompt_tokens,json=promptTokens,proto3" json:"prompt_tokens,omitempty"`
	CompletionTokens int32                  `protobuf:"varint,2,opt,name=completion_tokens,json=completionTokens,proto3" json:"completion_tokens,omitempty"`
	TotalTokens      int32                  `protobuf:"varint,3,opt,name=total_tokens,json=totalTokens,proto3" json:"total_tokens,omitempty"`
	unknownFields    protoimpl.UnknownFields
	sizeCache        protoimpl.SizeCache
}

func (x *Usage) Reset() {
	*x = Usage{}
	mi := &file_proto_inference_proto_msgTypes[6]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *Usage) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*Usage) ProtoMessage() {}

func (x *Usage) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[6]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use Usage.ProtoReflect.Descriptor instead.
func (*Usage) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{6}
}

func (x *Usage) GetPromptTokens() int32 {
	if x != nil {
		return x.PromptTokens
	}
	return 0
}

func (x *Usage) GetCompletionTokens() int32 {
	if x != nil {
		return x.CompletionTokens
	}
	return 0
}

func (x *Usage) GetTotalTokens() int32 {
	if x != nil {
		return x.TotalTokens
	}
	return 0
}

// Request for model status
type ModelStatusRequest struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	ModelId       string                 `protobuf:"bytes,1,opt,name=model_id,json=modelId,proto3" json:"model_id,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ModelStatusRequest) Reset() {
	*x = ModelStatusRequest{}
	mi := &file_proto_inference_proto_msgTypes[7]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ModelStatusRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ModelStatusRequest) ProtoMessage() {}

func (x *ModelStatusRequest) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[7]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ModelStatusRequest.ProtoReflect.Descriptor instead.
func (*ModelStatusRequest) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{7}
}

func (x *ModelStatusRequest) GetModelId() string {
	if x != nil {
		return x.ModelId
	}
	return ""
}

// Response for model status
type ModelStatusResponse struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	ModelId       string                 `protobuf:"bytes,1,opt,name=model_id,json=modelId,proto3" json:"model_id,omitempty"`
	Status        ModelStatus            `protobuf:"varint,2,opt,name=status,proto3,enum=helixflow.inference.ModelStatus" json:"status,omitempty"`
	LoadedAt      string                 `protobuf:"bytes,3,opt,name=loaded_at,json=loadedAt,proto3" json:"loaded_at,omitempty"`
	MemoryUsage   int64                  `protobuf:"varint,4,opt,name=memory_usage,json=memoryUsage,proto3" json:"memory_usage,omitempty"`
	GpuId         int32                  `protobuf:"varint,5,opt,name=gpu_id,json=gpuId,proto3" json:"gpu_id,omitempty"`
	Capabilities  []string               `protobuf:"bytes,6,rep,name=capabilities,proto3" json:"capabilities,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ModelStatusResponse) Reset() {
	*x = ModelStatusResponse{}
	mi := &file_proto_inference_proto_msgTypes[8]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ModelStatusResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ModelStatusResponse) ProtoMessage() {}

func (x *ModelStatusResponse) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[8]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ModelStatusResponse.ProtoReflect.Descriptor instead.
func (*ModelStatusResponse) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{8}
}

func (x *ModelStatusResponse) GetModelId() string {
	if x != nil {
		return x.ModelId
	}
	return ""
}

func (x *ModelStatusResponse) GetStatus() ModelStatus {
	if x != nil {
		return x.Status
	}
	return ModelStatus_MODEL_STATUS_UNKNOWN
}

func (x *ModelStatusResponse) GetLoadedAt() string {
	if x != nil {
		return x.LoadedAt
	}
	return ""
}

func (x *ModelStatusResponse) GetMemoryUsage() int64 {
	if x != nil {
		return x.MemoryUsage
	}
	return 0
}

func (x *ModelStatusResponse) GetGpuId() int32 {
	if x != nil {
		return x.GpuId
	}
	return 0
}

func (x *ModelStatusResponse) GetCapabilities() []string {
	if x != nil {
		return x.Capabilities
	}
	return nil
}

// Request to list models
type ListModelsRequest struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	Filter        string                 `protobuf:"bytes,1,opt,name=filter,proto3" json:"filter,omitempty"` // Optional filter for model types
	Limit         int32                  `protobuf:"varint,2,opt,name=limit,proto3" json:"limit,omitempty"`
	Offset        int32                  `protobuf:"varint,3,opt,name=offset,proto3" json:"offset,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ListModelsRequest) Reset() {
	*x = ListModelsRequest{}
	mi := &file_proto_inference_proto_msgTypes[9]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ListModelsRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ListModelsRequest) ProtoMessage() {}

func (x *ListModelsRequest) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[9]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ListModelsRequest.ProtoReflect.Descriptor instead.
func (*ListModelsRequest) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{9}
}

func (x *ListModelsRequest) GetFilter() string {
	if x != nil {
		return x.Filter
	}
	return ""
}

func (x *ListModelsRequest) GetLimit() int32 {
	if x != nil {
		return x.Limit
	}
	return 0
}

func (x *ListModelsRequest) GetOffset() int32 {
	if x != nil {
		return x.Offset
	}
	return 0
}

// Response with model list
type ListModelsResponse struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	Models        []*ModelInfo           `protobuf:"bytes,1,rep,name=models,proto3" json:"models,omitempty"`
	TotalCount    int32                  `protobuf:"varint,2,opt,name=total_count,json=totalCount,proto3" json:"total_count,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ListModelsResponse) Reset() {
	*x = ListModelsResponse{}
	mi := &file_proto_inference_proto_msgTypes[10]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ListModelsResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ListModelsResponse) ProtoMessage() {}

func (x *ListModelsResponse) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[10]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ListModelsResponse.ProtoReflect.Descriptor instead.
func (*ListModelsResponse) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{10}
}

func (x *ListModelsResponse) GetModels() []*ModelInfo {
	if x != nil {
		return x.Models
	}
	return nil
}

func (x *ListModelsResponse) GetTotalCount() int32 {
	if x != nil {
		return x.TotalCount
	}
	return 0
}

// Model information
type ModelInfo struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	Id            string                 `protobuf:"bytes,1,opt,name=id,proto3" json:"id,omitempty"`
	Name          string                 `protobuf:"bytes,2,opt,name=name,proto3" json:"name,omitempty"`
	Provider      string                 `protobuf:"bytes,3,opt,name=provider,proto3" json:"provider,omitempty"`
	Version       string                 `protobuf:"bytes,4,opt,name=version,proto3" json:"version,omitempty"`
	Capabilities  []string               `protobuf:"bytes,5,rep,name=capabilities,proto3" json:"capabilities,omitempty"`
	Size          int64                  `protobuf:"varint,6,opt,name=size,proto3" json:"size,omitempty"`
	Description   string                 `protobuf:"bytes,7,opt,name=description,proto3" json:"description,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ModelInfo) Reset() {
	*x = ModelInfo{}
	mi := &file_proto_inference_proto_msgTypes[11]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ModelInfo) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ModelInfo) ProtoMessage() {}

func (x *ModelInfo) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[11]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ModelInfo.ProtoReflect.Descriptor instead.
func (*ModelInfo) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{11}
}

func (x *ModelInfo) GetId() string {
	if x != nil {
		return x.Id
	}
	return ""
}

func (x *ModelInfo) GetName() string {
	if x != nil {
		return x.Name
	}
	return ""
}

func (x *ModelInfo) GetProvider() string {
	if x != nil {
		return x.Provider
	}
	return ""
}

func (x *ModelInfo) GetVersion() string {
	if x != nil {
		return x.Version
	}
	return ""
}

func (x *ModelInfo) GetCapabilities() []string {
	if x != nil {
		return x.Capabilities
	}
	return nil
}

func (x *ModelInfo) GetSize() int64 {
	if x != nil {
		return x.Size
	}
	return 0
}

func (x *ModelInfo) GetDescription() string {
	if x != nil {
		return x.Description
	}
	return ""
}

// Request to load a model
type LoadModelRequest struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	ModelId       string                 `protobuf:"bytes,1,opt,name=model_id,json=modelId,proto3" json:"model_id,omitempty"`
	GpuId         int32                  `protobuf:"varint,2,opt,name=gpu_id,json=gpuId,proto3" json:"gpu_id,omitempty"` // Optional specific GPU
	Config        map[string]string      `protobuf:"bytes,3,rep,name=config,proto3" json:"config,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *LoadModelRequest) Reset() {
	*x = LoadModelRequest{}
	mi := &file_proto_inference_proto_msgTypes[12]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *LoadModelRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*LoadModelRequest) ProtoMessage() {}

func (x *LoadModelRequest) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[12]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use LoadModelRequest.ProtoReflect.Descriptor instead.
func (*LoadModelRequest) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{12}
}

func (x *LoadModelRequest) GetModelId() string {
	if x != nil {
		return x.ModelId
	}
	return ""
}

func (x *LoadModelRequest) GetGpuId() int32 {
	if x != nil {
		return x.GpuId
	}
	return 0
}

func (x *LoadModelRequest) GetConfig() map[string]string {
	if x != nil {
		return x.Config
	}
	return nil
}

// Response for model loading
type LoadModelResponse struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	ModelId       string                 `protobuf:"bytes,1,opt,name=model_id,json=modelId,proto3" json:"model_id,omitempty"`
	Success       bool                   `protobuf:"varint,2,opt,name=success,proto3" json:"success,omitempty"`
	Message       string                 `protobuf:"bytes,3,opt,name=message,proto3" json:"message,omitempty"`
	LoadTimeMs    int64                  `protobuf:"varint,4,opt,name=load_time_ms,json=loadTimeMs,proto3" json:"load_time_ms,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *LoadModelResponse) Reset() {
	*x = LoadModelResponse{}
	mi := &file_proto_inference_proto_msgTypes[13]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *LoadModelResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*LoadModelResponse) ProtoMessage() {}

func (x *LoadModelResponse) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[13]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use LoadModelResponse.ProtoReflect.Descriptor instead.
func (*LoadModelResponse) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{13}
}

func (x *LoadModelResponse) GetModelId() string {
	if x != nil {
		return x.ModelId
	}
	return ""
}

func (x *LoadModelResponse) GetSuccess() bool {
	if x != nil {
		return x.Success
	}
	return false
}

func (x *LoadModelResponse) GetMessage() string {
	if x != nil {
		return x.Message
	}
	return ""
}

func (x *LoadModelResponse) GetLoadTimeMs() int64 {
	if x != nil {
		return x.LoadTimeMs
	}
	return 0
}

// Request to unload a model
type UnloadModelRequest struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	ModelId       string                 `protobuf:"bytes,1,opt,name=model_id,json=modelId,proto3" json:"model_id,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *UnloadModelRequest) Reset() {
	*x = UnloadModelRequest{}
	mi := &file_proto_inference_proto_msgTypes[14]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *UnloadModelRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*UnloadModelRequest) ProtoMessage() {}

func (x *UnloadModelRequest) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[14]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use UnloadModelRequest.ProtoReflect.Descriptor instead.
func (*UnloadModelRequest) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{14}
}

func (x *UnloadModelRequest) GetModelId() string {
	if x != nil {
		return x.ModelId
	}
	return ""
}

// Response for model unloading
type UnloadModelResponse struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	ModelId       string                 `protobuf:"bytes,1,opt,name=model_id,json=modelId,proto3" json:"model_id,omitempty"`
	Success       bool                   `protobuf:"varint,2,opt,name=success,proto3" json:"success,omitempty"`
	Message       string                 `protobuf:"bytes,3,opt,name=message,proto3" json:"message,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *UnloadModelResponse) Reset() {
	*x = UnloadModelResponse{}
	mi := &file_proto_inference_proto_msgTypes[15]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *UnloadModelResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*UnloadModelResponse) ProtoMessage() {}

func (x *UnloadModelResponse) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[15]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use UnloadModelResponse.ProtoReflect.Descriptor instead.
func (*UnloadModelResponse) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{15}
}

func (x *UnloadModelResponse) GetModelId() string {
	if x != nil {
		return x.ModelId
	}
	return ""
}

func (x *UnloadModelResponse) GetSuccess() bool {
	if x != nil {
		return x.Success
	}
	return false
}

func (x *UnloadModelResponse) GetMessage() string {
	if x != nil {
		return x.Message
	}
	return ""
}

// Request for system status
type SystemStatusRequest struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *SystemStatusRequest) Reset() {
	*x = SystemStatusRequest{}
	mi := &file_proto_inference_proto_msgTypes[16]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *SystemStatusRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*SystemStatusRequest) ProtoMessage() {}

func (x *SystemStatusRequest) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[16]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use SystemStatusRequest.ProtoReflect.Descriptor instead.
func (*SystemStatusRequest) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{16}
}

// Response with system status
type SystemStatusResponse struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	Gpus          []*GPUInfo             `protobuf:"bytes,1,rep,name=gpus,proto3" json:"gpus,omitempty"`
	Resources     *SystemResources       `protobuf:"bytes,2,opt,name=resources,proto3" json:"resources,omitempty"`
	LoadedModels  []*LoadedModel         `protobuf:"bytes,3,rep,name=loaded_models,json=loadedModels,proto3" json:"loaded_models,omitempty"`
	UptimeSeconds int64                  `protobuf:"varint,4,opt,name=uptime_seconds,json=uptimeSeconds,proto3" json:"uptime_seconds,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *SystemStatusResponse) Reset() {
	*x = SystemStatusResponse{}
	mi := &file_proto_inference_proto_msgTypes[17]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *SystemStatusResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*SystemStatusResponse) ProtoMessage() {}

func (x *SystemStatusResponse) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[17]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use SystemStatusResponse.ProtoReflect.Descriptor instead.
func (*SystemStatusResponse) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{17}
}

func (x *SystemStatusResponse) GetGpus() []*GPUInfo {
	if x != nil {
		return x.Gpus
	}
	return nil
}

func (x *SystemStatusResponse) GetResources() *SystemResources {
	if x != nil {
		return x.Resources
	}
	return nil
}

func (x *SystemStatusResponse) GetLoadedModels() []*LoadedModel {
	if x != nil {
		return x.LoadedModels
	}
	return nil
}

func (x *SystemStatusResponse) GetUptimeSeconds() int64 {
	if x != nil {
		return x.UptimeSeconds
	}
	return 0
}

// GPU information
type GPUInfo struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	Id            int32                  `protobuf:"varint,1,opt,name=id,proto3" json:"id,omitempty"`
	Name          string                 `protobuf:"bytes,2,opt,name=name,proto3" json:"name,omitempty"`
	MemoryTotal   int64                  `protobuf:"varint,3,opt,name=memory_total,json=memoryTotal,proto3" json:"memory_total,omitempty"`
	MemoryUsed    int64                  `protobuf:"varint,4,opt,name=memory_used,json=memoryUsed,proto3" json:"memory_used,omitempty"`
	Utilization   float64                `protobuf:"fixed64,5,opt,name=utilization,proto3" json:"utilization,omitempty"`
	Temperature   float64                `protobuf:"fixed64,6,opt,name=temperature,proto3" json:"temperature,omitempty"`
	Available     bool                   `protobuf:"varint,7,opt,name=available,proto3" json:"available,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *GPUInfo) Reset() {
	*x = GPUInfo{}
	mi := &file_proto_inference_proto_msgTypes[18]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *GPUInfo) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*GPUInfo) ProtoMessage() {}

func (x *GPUInfo) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[18]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use GPUInfo.ProtoReflect.Descriptor instead.
func (*GPUInfo) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{18}
}

func (x *GPUInfo) GetId() int32 {
	if x != nil {
		return x.Id
	}
	return 0
}

func (x *GPUInfo) GetName() string {
	if x != nil {
		return x.Name
	}
	return ""
}

func (x *GPUInfo) GetMemoryTotal() int64 {
	if x != nil {
		return x.MemoryTotal
	}
	return 0
}

func (x *GPUInfo) GetMemoryUsed() int64 {
	if x != nil {
		return x.MemoryUsed
	}
	return 0
}

func (x *GPUInfo) GetUtilization() float64 {
	if x != nil {
		return x.Utilization
	}
	return 0
}

func (x *GPUInfo) GetTemperature() float64 {
	if x != nil {
		return x.Temperature
	}
	return 0
}

func (x *GPUInfo) GetAvailable() bool {
	if x != nil {
		return x.Available
	}
	return false
}

// System resources
type SystemResources struct {
	state          protoimpl.MessageState `protogen:"open.v1"`
	MemoryTotal    int64                  `protobuf:"varint,1,opt,name=memory_total,json=memoryTotal,proto3" json:"memory_total,omitempty"`
	MemoryUsed     int64                  `protobuf:"varint,2,opt,name=memory_used,json=memoryUsed,proto3" json:"memory_used,omitempty"`
	CpuUtilization float64                `protobuf:"fixed64,3,opt,name=cpu_utilization,json=cpuUtilization,proto3" json:"cpu_utilization,omitempty"`
	DiskTotal      int64                  `protobuf:"varint,4,opt,name=disk_total,json=diskTotal,proto3" json:"disk_total,omitempty"`
	DiskUsed       int64                  `protobuf:"varint,5,opt,name=disk_used,json=diskUsed,proto3" json:"disk_used,omitempty"`
	unknownFields  protoimpl.UnknownFields
	sizeCache      protoimpl.SizeCache
}

func (x *SystemResources) Reset() {
	*x = SystemResources{}
	mi := &file_proto_inference_proto_msgTypes[19]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *SystemResources) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*SystemResources) ProtoMessage() {}

func (x *SystemResources) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[19]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use SystemResources.ProtoReflect.Descriptor instead.
func (*SystemResources) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{19}
}

func (x *SystemResources) GetMemoryTotal() int64 {
	if x != nil {
		return x.MemoryTotal
	}
	return 0
}

func (x *SystemResources) GetMemoryUsed() int64 {
	if x != nil {
		return x.MemoryUsed
	}
	return 0
}

func (x *SystemResources) GetCpuUtilization() float64 {
	if x != nil {
		return x.CpuUtilization
	}
	return 0
}

func (x *SystemResources) GetDiskTotal() int64 {
	if x != nil {
		return x.DiskTotal
	}
	return 0
}

func (x *SystemResources) GetDiskUsed() int64 {
	if x != nil {
		return x.DiskUsed
	}
	return 0
}

// Currently loaded model
type LoadedModel struct {
	state          protoimpl.MessageState `protogen:"open.v1"`
	ModelId        string                 `protobuf:"bytes,1,opt,name=model_id,json=modelId,proto3" json:"model_id,omitempty"`
	GpuId          int32                  `protobuf:"varint,2,opt,name=gpu_id,json=gpuId,proto3" json:"gpu_id,omitempty"`
	MemoryUsage    int64                  `protobuf:"varint,3,opt,name=memory_usage,json=memoryUsage,proto3" json:"memory_usage,omitempty"`
	LoadTime       int64                  `protobuf:"varint,4,opt,name=load_time,json=loadTime,proto3" json:"load_time,omitempty"`
	ActiveRequests int32                  `protobuf:"varint,5,opt,name=active_requests,json=activeRequests,proto3" json:"active_requests,omitempty"`
	unknownFields  protoimpl.UnknownFields
	sizeCache      protoimpl.SizeCache
}

func (x *LoadedModel) Reset() {
	*x = LoadedModel{}
	mi := &file_proto_inference_proto_msgTypes[20]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *LoadedModel) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*LoadedModel) ProtoMessage() {}

func (x *LoadedModel) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[20]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use LoadedModel.ProtoReflect.Descriptor instead.
func (*LoadedModel) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{20}
}

func (x *LoadedModel) GetModelId() string {
	if x != nil {
		return x.ModelId
	}
	return ""
}

func (x *LoadedModel) GetGpuId() int32 {
	if x != nil {
		return x.GpuId
	}
	return 0
}

func (x *LoadedModel) GetMemoryUsage() int64 {
	if x != nil {
		return x.MemoryUsage
	}
	return 0
}

func (x *LoadedModel) GetLoadTime() int64 {
	if x != nil {
		return x.LoadTime
	}
	return 0
}

func (x *LoadedModel) GetActiveRequests() int32 {
	if x != nil {
		return x.ActiveRequests
	}
	return 0
}

var File_proto_inference_proto protoreflect.FileDescriptor

const file_proto_inference_proto_rawDesc = "" +
	"\n" +
	"\x15proto/inference.proto\x12\x13helixflow.inference\"\x94\x03\n" +
	"\x10InferenceRequest\x12\x19\n" +
	"\bmodel_id\x18\x01 \x01(\tR\amodelId\x12\x17\n" +
	"\auser_id\x18\x02 \x01(\tR\x06userId\x12<\n" +
	"\bmessages\x18\x03 \x03(\v2 .helixflow.inference.ChatMessageR\bmessages\x12\x1d\n" +
	"\n" +
	"max_tokens\x18\x04 \x01(\x05R\tmaxTokens\x12 \n" +
	"\vtemperature\x18\x05 \x01(\x01R\vtemperature\x12\x13\n" +
	"\x05top_p\x18\x06 \x01(\x01R\x04topP\x12\x12\n" +
	"\x04stop\x18\a \x03(\tR\x04stop\x12\x16\n" +
	"\x06stream\x18\b \x01(\bR\x06stream\x12O\n" +
	"\bmetadata\x18\t \x03(\v23.helixflow.inference.InferenceRequest.MetadataEntryR\bmetadata\x1a;\n" +
	"\rMetadataEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n" +
	"\x05value\x18\x02 \x01(\tR\x05value:\x028\x01\"O\n" +
	"\vChatMessage\x12\x12\n" +
	"\x04role\x18\x01 \x01(\tR\x04role\x12\x18\n" +
	"\acontent\x18\x02 \x01(\tR\acontent\x12\x12\n" +
	"\x04name\x18\x03 \x01(\tR\x04name\"\xf9\x01\n" +
	"\x11InferenceResponse\x12\x0e\n" +
	"\x02id\x18\x01 \x01(\tR\x02id\x12\x16\n" +
	"\x06object\x18\x02 \x01(\tR\x06object\x12\x18\n" +
	"\acreated\x18\x03 \x01(\x03R\acreated\x12\x14\n" +
	"\x05model\x18\x04 \x01(\tR\x05model\x125\n" +
	"\achoices\x18\x05 \x03(\v2\x1b.helixflow.inference.ChoiceR\achoices\x120\n" +
	"\x05usage\x18\x06 \x01(\v2\x1a.helixflow.inference.UsageR\x05usage\x12#\n" +
	"\rfinish_reason\x18\a \x01(\tR\ffinishReason\"\x9f\x01\n" +
	"\x0eInferenceChunk\x12\x0e\n" +
	"\x02id\x18\x01 \x01(\tR\x02id\x12\x16\n" +
	"\x06object\x18\x02 \x01(\tR\x06object\x12\x18\n" +
	"\acreated\x18\x03 \x01(\x03R\acreated\x12\x14\n" +
	"\x05model\x18\x04 \x01(\tR\x05model\x125\n" +
	"\achoices\x18\x05 \x03(\v2\x1b.helixflow.inference.ChoiceR\achoices\"\xb1\x01\n" +
	"\x06Choice\x12\x14\n" +
	"\x05index\x18\x01 \x01(\x05R\x05index\x12:\n" +
	"\amessage\x18\x02 \x01(\v2 .helixflow.inference.ChatMessageR\amessage\x12#\n" +
	"\rfinish_reason\x18\x03 \x01(\tR\ffinishReason\x120\n" +
	"\x05delta\x18\x04 \x01(\v2\x1a.helixflow.inference.DeltaR\x05delta\"5\n" +
	"\x05Delta\x12\x12\n" +
	"\x04role\x18\x01 \x01(\tR\x04role\x12\x18\n" +
	"\acontent\x18\x02 \x01(\tR\acontent\"|\n" +
	"\x05Usage\x12#\n" +
	"\rprompt_tokens\x18\x01 \x01(\x05R\fpromptTokens\x12+\n" +
	"\x11completion_tokens\x18\x02 \x01(\x05R\x10completionTokens\x12!\n" +
	"\ftotal_tokens\x18\x03 \x01(\x05R\vtotalTokens\"/\n" +
	"\x12ModelStatusRequest\x12\x19\n" +
	"\bmodel_id\x18\x01 \x01(\tR\amodelId\"\xe5\x01\n" +
	"\x13ModelStatusResponse\x12\x19\n" +
	"\bmodel_id\x18\x01 \x01(\tR\amodelId\x128\n" +
	"\x06status\x18\x02 \x01(\x0e2 .helixflow.inference.ModelStatusR\x06status\x12\x1b\n" +
	"\tloaded_at\x18\x03 \x01(\tR\bloadedAt\x12!\n" +
	"\fmemory_usage\x18\x04 \x01(\x03R\vmemoryUsage\x12\x15\n" +
	"\x06gpu_id\x18\x05 \x01(\x05R\x05gpuId\x12\"\n" +
	"\fcapabilities\x18\x06 \x03(\tR\fcapabilities\"Y\n" +
	"\x11ListModelsRequest\x12\x16\n" +
	"\x06filter\x18\x01 \x01(\tR\x06filter\x12\x14\n" +
	"\x05limit\x18\x02 \x01(\x05R\x05limit\x12\x16\n" +
	"\x06offset\x18\x03 \x01(\x05R\x06offset\"m\n" +
	"\x12ListModelsResponse\x126\n" +
	"\x06models\x18\x01 \x03(\v2\x1e.helixflow.inference.ModelInfoR\x06models\x12\x1f\n" +
	"\vtotal_count\x18\x02 \x01(\x05R\n" +
	"totalCount\"\xbf\x01\n" +
	"\tModelInfo\x12\x0e\n" +
	"\x02id\x18\x01 \x01(\tR\x02id\x12\x12\n" +
	"\x04name\x18\x02 \x01(\tR\x04name\x12\x1a\n" +
	"\bprovider\x18\x03 \x01(\tR\bprovider\x12\x18\n" +
	"\aversion\x18\x04 \x01(\tR\aversion\x12\"\n" +
	"\fcapabilities\x18\x05 \x03(\tR\fcapabilities\x12\x12\n" +
	"\x04size\x18\x06 \x01(\x03R\x04size\x12 \n" +
	"\vdescription\x18\a \x01(\tR\vdescription\"\xca\x01\n" +
	"\x10LoadModelRequest\x12\x19\n" +
	"\bmodel_id\x18\x01 \x01(\tR\amodelId\x12\x15\n" +
	"\x06gpu_id\x18\x02 \x01(\x05R\x05gpuId\x12I\n" +
	"\x06config\x18\x03 \x03(\v21.helixflow.inference.LoadModelRequest.ConfigEntryR\x06config\x1a9\n" +
	"\vConfigEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n" +
	"\x05value\x18\x02 \x01(\tR\x05value:\x028\x01\"\x84\x01\n" +
	"\x11LoadModelResponse\x12\x19\n" +
	"\bmodel_id\x18\x01 \x01(\tR\amodelId\x12\x18\n" +
	"\asuccess\x18\x02 \x01(\bR\asuccess\x12\x18\n" +
	"\amessage\x18\x03 \x01(\tR\amessage\x12 \n" +
	"\fload_time_ms\x18\x04 \x01(\x03R\n" +
	"loadTimeMs\"/\n" +
	"\x12UnloadModelRequest\x12\x19\n" +
	"\bmodel_id\x18\x01 \x01(\tR\amodelId\"d\n" +
	"\x13UnloadModelResponse\x12\x19\n" +
	"\bmodel_id\x18\x01 \x01(\tR\amodelId\x12\x18\n" +
	"\asuccess\x18\x02 \x01(\bR\asuccess\x12\x18\n" +
	"\amessage\x18\x03 \x01(\tR\amessage\"\x15\n" +
	"\x13SystemStatusRequest\"\xfa\x01\n" +
	"\x14SystemStatusResponse\x120\n" +
	"\x04gpus\x18\x01 \x03(\v2\x1c.helixflow.inference.GPUInfoR\x04gpus\x12B\n" +
	"\tresources\x18\x02 \x01(\v2$.helixflow.inference.SystemResourcesR\tresources\x12E\n" +
	"\rloaded_models\x18\x03 \x03(\v2 .helixflow.inference.LoadedModelR\floadedModels\x12%\n" +
	"\x0euptime_seconds\x18\x04 \x01(\x03R\ruptimeSeconds\"\xd3\x01\n" +
	"\aGPUInfo\x12\x0e\n" +
	"\x02id\x18\x01 \x01(\x05R\x02id\x12\x12\n" +
	"\x04name\x18\x02 \x01(\tR\x04name\x12!\n" +
	"\fmemory_total\x18\x03 \x01(\x03R\vmemoryTotal\x12\x1f\n" +
	"\vmemory_used\x18\x04 \x01(\x03R\n" +
	"memoryUsed\x12 \n" +
	"\vutilization\x18\x05 \x01(\x01R\vutilization\x12 \n" +
	"\vtemperature\x18\x06 \x01(\x01R\vtemperature\x12\x1c\n" +
	"\tavailable\x18\a \x01(\bR\tavailable\"\xba\x01\n" +
	"\x0fSystemResources\x12!\n" +
	"\fmemory_total\x18\x01 \x01(\x03R\vmemoryTotal\x12\x1f\n" +
	"\vmemory_used\x18\x02 \x01(\x03R\n" +
	"memoryUsed\x12'\n" +
	"\x0fcpu_utilization\x18\x03 \x01(\x01R\x0ecpuUtilization\x12\x1d\n" +
	"\n" +
	"disk_total\x18\x04 \x01(\x03R\tdiskTotal\x12\x1b\n" +
	"\tdisk_used\x18\x05 \x01(\x03R\bdiskUsed\"\xa8\x01\n" +
	"\vLoadedModel\x12\x19\n" +
	"\bmodel_id\x18\x01 \x01(\tR\amodelId\x12\x15\n" +
	"\x06gpu_id\x18\x02 \x01(\x05R\x05gpuId\x12!\n" +
	"\fmemory_usage\x18\x03 \x01(\x03R\vmemoryUsage\x12\x1b\n" +
	"\tload_time\x18\x04 \x01(\x03R\bloadTime\x12'\n" +
	"\x0factive_requests\x18\x05 \x01(\x05R\x0eactiveRequests*\x8c\x01\n" +
	"\vModelStatus\x12\x18\n" +
	"\x14MODEL_STATUS_UNKNOWN\x10\x00\x12\x18\n" +
	"\x14MODEL_STATUS_LOADING\x10\x01\x12\x16\n" +
	"\x12MODEL_STATUS_READY\x10\x02\x12\x16\n" +
	"\x12MODEL_STATUS_ERROR\x10\x03\x12\x19\n" +
	"\x15MODEL_STATUS_UNLOADED\x10\x042\xb9\x05\n" +
	"\x10InferenceService\x12Z\n" +
	"\tInference\x12%.helixflow.inference.InferenceRequest\x1a&.helixflow.inference.InferenceResponse\x12_\n" +
	"\x0fStreamInference\x12%.helixflow.inference.InferenceRequest\x1a#.helixflow.inference.InferenceChunk0\x01\x12c\n" +
	"\x0eGetModelStatus\x12'.helixflow.inference.ModelStatusRequest\x1a(.helixflow.inference.ModelStatusResponse\x12]\n" +
	"\n" +
	"ListModels\x12&.helixflow.inference.ListModelsRequest\x1a'.helixflow.inference.ListModelsResponse\x12Z\n" +
	"\tLoadModel\x12%.helixflow.inference.LoadModelRequest\x1a&.helixflow.inference.LoadModelResponse\x12`\n" +
	"\vUnloadModel\x12'.helixflow.inference.UnloadModelRequest\x1a(.helixflow.inference.UnloadModelResponse\x12f\n" +
	"\x0fGetSystemStatus\x12(.helixflow.inference.SystemStatusRequest\x1a).helixflow.inference.SystemStatusResponseB\x15Z\x13helixflow/inferenceb\x06proto3"

var (
	file_proto_inference_proto_rawDescOnce sync.Once
	file_proto_inference_proto_rawDescData []byte
)

func file_proto_inference_proto_rawDescGZIP() []byte {
	file_proto_inference_proto_rawDescOnce.Do(func() {
		file_proto_inference_proto_rawDescData = protoimpl.X.CompressGZIP(unsafe.Slice(unsafe.StringData(file_proto_inference_proto_rawDesc), len(file_proto_inference_proto_rawDesc)))
	})
	return file_proto_inference_proto_rawDescData
}

var file_proto_inference_proto_enumTypes = make([]protoimpl.EnumInfo, 1)
var file_proto_inference_proto_msgTypes = make([]protoimpl.MessageInfo, 23)
var file_proto_inference_proto_goTypes = []any{
	(ModelStatus)(0),             // 0: helixflow.inference.ModelStatus
	(*InferenceRequest)(nil),     // 1: helixflow.inference.InferenceRequest
	(*ChatMessage)(nil),          // 2: helixflow.inference.ChatMessage
	(*InferenceResponse)(nil),    // 3: helixflow.inference.InferenceResponse
	(*InferenceChunk)(nil),       // 4: helixflow.inference.InferenceChunk
	(*Choice)(nil),               // 5: helixflow.inference.Choice
	(*Delta)(nil),                // 6: helixflow.inference.Delta
	(*Usage)(nil),                // 7: helixflow.inference.Usage
	(*ModelStatusRequest)(nil),   // 8: helixflow.inference.ModelStatusRequest
	(*ModelStatusResponse)(nil),  // 9: helixflow.inference.ModelStatusResponse
	(*ListModelsRequest)(nil),    // 10: helixflow.inference.ListModelsRequest
	(*ListModelsResponse)(nil),   // 11: helixflow.inference.ListModelsResponse
	(*ModelInfo)(nil),            // 12: helixflow.inference.ModelInfo
	(*LoadModelRequest)(nil),     // 13: helixflow.inference.LoadModelRequest
	(*LoadModelResponse)(nil),    // 14: helixflow.inference.LoadModelResponse
	(*UnloadModelRequest)(nil),   // 15: helixflow.inference.UnloadModelRequest
	(*UnloadModelResponse)(nil),  // 16: helixflow.inference.UnloadModelResponse
	(*SystemStatusRequest)(nil),  // 17: helixflow.inference.SystemStatusRequest
	(*SystemStatusResponse)(nil), // 18: helixflow.inference.SystemStatusResponse
	(*GPUInfo)(nil),              // 19: helixflow.inference.GPUInfo
	(*SystemResources)(nil),      // 20: helixflow.inference.SystemResources
	(*LoadedModel)(nil),          // 21: helixflow.inference.LoadedModel
	nil,                          // 22: helixflow.inference.InferenceRequest.MetadataEntry
	nil,                          // 23: helixflow.inference.LoadModelRequest.ConfigEntry
}
var file_proto_inference_proto_depIdxs = []int32{
	2,  // 0: helixflow.inference.InferenceRequest.messages:type_name -> helixflow.inference.ChatMessage
	22, // 1: helixflow.inference.InferenceRequest.metadata:type_name -> helixflow.inference.InferenceRequest.MetadataEntry
	5,  // 2: helixflow.inference.InferenceResponse.choices:type_name -> helixflow.inference.Choice
	7,  // 3: helixflow.inference.InferenceResponse.usage:type_name -> helixflow.inference.Usage
	5,  // 4: helixflow.inference.InferenceChunk.choices:type_name -> helixflow.inference.Choice
	2,  // 5: helixflow.inference.Choice.message:type_name -> helixflow.inference.ChatMessage
	6,  // 6: helixflow.inference.Choice.delta:type_name -> helixflow.inference.Delta
	0,  // 7: helixflow.inference.ModelStatusResponse.status:type_name -> helixflow.inference.ModelStatus
	12, // 8: helixflow.inference.ListModelsResponse.models:type_name -> helixflow.inference.ModelInfo
	23, // 9: helixflow.inference.LoadModelRequest.config:type_name -> helixflow.inference.LoadModelRequest.ConfigEntry
	19, // 10: helixflow.inference.SystemStatusResponse.gpus:type_name -> helixflow.inference.GPUInfo
	20, // 11: helixflow.inference.SystemStatusResponse.resources:type_name -> helixflow.inference.SystemResources
	21, // 12: helixflow.inference.SystemStatusResponse.loaded_models:type_name -> helixflow.inference.LoadedModel
	1,  // 13: helixflow.inference.InferenceService.Inference:input_type -> helixflow.inference.InferenceRequest
	1,  // 14: helixflow.inference.InferenceService.StreamInference:input_type -> helixflow.inference.InferenceRequest
	8,  // 15: helixflow.inference.InferenceService.GetModelStatus:input_type -> helixflow.inference.ModelStatusRequest
	10, // 16: helixflow.inference.InferenceService.ListModels:input_type -> helixflow.inference.ListModelsRequest
	13, // 17: helixflow.inference.InferenceService.LoadModel:input_type -> helixflow.inference.LoadModelRequest
	15, // 18: helixflow.inference.InferenceService.UnloadModel:input_type -> helixflow.inference.UnloadModelRequest
	17, // 19: helixflow.inference.InferenceService.GetSystemStatus:input_type -> helixflow.inference.SystemStatusRequest
	3,  // 20: helixflow.inference.InferenceService.Inference:output_type -> helixflow.inference.InferenceResponse
	4,  // 21: helixflow.inference.InferenceService.StreamInference:output_type -> helixflow.inference.InferenceChunk
	9,  // 22: helixflow.inference.InferenceService.GetModelStatus:output_type -> helixflow.inference.ModelStatusResponse
	11, // 23: helixflow.inference.InferenceService.ListModels:output_type -> helixflow.inference.ListModelsResponse
	14, // 24: helixflow.inference.InferenceService.LoadModel:output_type -> helixflow.inference.LoadModelResponse
	16, // 25: helixflow.inference.InferenceService.UnloadModel:output_type -> helixflow.inference.UnloadModelResponse
	18, // 26: helixflow.inference.InferenceService.GetSystemStatus:output_type -> helixflow.inference.SystemStatusResponse
	20, // [20:27] is the sub-list for method output_type
	13, // [13:20] is the sub-list for method input_type
	13, // [13:13] is the sub-list for extension type_name
	13, // [13:13] is the sub-list for extension extendee
	0,  // [0:13] is the sub-list for field type_name
}

func init() { file_proto_inference_proto_init() }
func file_proto_inference_proto_init() {
	if File_proto_inference_proto != nil {
		return
	}
	type x struct{}
	out := protoimpl.TypeBuilder{
		File: protoimpl.DescBuilder{
			GoPackagePath: reflect.TypeOf(x{}).PkgPath(),
			RawDescriptor: unsafe.Slice(unsafe.StringData(file_proto_inference_proto_rawDesc), len(file_proto_inference_proto_rawDesc)),
			NumEnums:      1,
			NumMessages:   23,
			NumExtensions: 0,
			NumServices:   1,
		},
		GoTypes:           file_proto_inference_proto_goTypes,
		DependencyIndexes: file_proto_inference_proto_depIdxs,
		EnumInfos:         file_proto_inference_proto_enumTypes,
		MessageInfos:      file_proto_inference_proto_msgTypes,
	}.Build()
	File_proto_inference_proto = out.File
	file_proto_inference_proto_goTypes = nil
	file_proto_inference_proto_depIdxs = nil
}
